{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 300 Autoencoder Tutorial Notebook\n",
    "\n",
    "This Notebook should give a quick introduction to how Autoencoders work. It will be trained on the MNIST handwritten digit dataset. The code is dependant on keras, matplotlib and numpy while using Tensorflow 2.X.\n",
    "\n",
    "\n",
    "## Sources:\n",
    "MNIST Database:\n",
    " - http://yann.lecun.com/exdb/mnist/\n",
    "\n",
    "Papers:\n",
    " - https://arxiv.org/abs/1805.00251\n",
    " - https://arxiv.org/abs/1805.09730\n",
    "\n",
    "Tutorials:\n",
    " - https://blog.keras.io/building-autoencoders-in-keras.html\n",
    " \n",
    " \n",
    " ## Training the Autoencoder\n",
    " \n",
    " The Autoencoder is composed of an encoder and a decoder half. Once an MNIST image is fed into the network, it is compressed to the *feature_map* layer, which maps the small input space to the generated MNIST images. The image fed into the autoencoder and outputted by it should be identical. The loss function penalizes the network when there is a big difference.\n",
    " \n",
    " <img src=\"Autoencoder MNIST Explanation.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1)\n",
      "(60000,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x13a446ce808>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Reshape, Dense, Flatten, LeakyReLU\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "# Expand Dimensions to 3D to add one for the color channel of black and white: expand_dims(train_images, axis=-1)\n",
    "# Normalize the image by dividing by 255 --> faster convergence\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "train_images, test_images = np.expand_dims(train_images, axis=-1) / 255.0, np.expand_dims(train_images, axis=-1) / 255.0\n",
    "\n",
    "\n",
    "# Examples\n",
    "# Shape is (60000, 28, 28, 1) for 60 000 images with dimensions 28 x 28 and 1 color channel (black and white)\n",
    "print(train_images.shape)\n",
    "print(train_labels.shape)\n",
    "\n",
    "plt.imshow(train_images[0, :, :, 0], cmap='gray_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x13a4472f748>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Helper function to create a batch\n",
    "# It returns identical image pairs: *train_images* = *y_train_images*\n",
    "\n",
    "# Sample minibatch from MNIST\n",
    "def get_MNIST_samples(n_samples):\n",
    "    # Pick new image samples from dataset\n",
    "    # Pick random images to help with gradient descent\n",
    "    image_samples = np.zeros((n_samples, 28, 28, 1))\n",
    "\n",
    "    for i in range(0, n_samples):\n",
    "        index = random.randint(0, 59999)\n",
    "        image_samples[i] = train_images[index]\n",
    "\n",
    "    # Both x and y are the same, since the autoencoder should produce the same image\n",
    "    return image_samples, image_samples\n",
    "\n",
    "\n",
    "# Examples\n",
    "train_images_example, y_train_images_example = get_MNIST_samples(32)\n",
    "plt.imshow(train_images_example[0, :, :, 0], cmap='gray_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Encoder, Decoder and the whole Autoencoder with Keras\n",
    "\n",
    "# Build the encoder\n",
    "def build_encoder():\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Flatten(input_shape=(28, 28, 1)))\n",
    "\n",
    "    model.add(Dense(256))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "    model.add(Dense(128))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "    model.add(Dense(64))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Build the decoder\n",
    "def build_decoder():\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(2))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "    model.add(Dense(64))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "    model.add(Dense(128))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "    model.add(Dense(256))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "    # activation='sigmoid' to get the values for the image between 0 and 1\n",
    "    model.add(Dense(784, activation='sigmoid'))\n",
    "\n",
    "    # None ist the batch size, 28 x 28 is the input image with 1 color channel\n",
    "    model.add(Reshape((28, 28, 1)))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Build the Autoencoder by combining the encoder and the decoder\n",
    "def build_autoencoder(encoder, decoder):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(encoder)\n",
    "    model.add(decoder)\n",
    "\n",
    "    # Loss function and optimizer\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "def train(encoder, decoder, autoencoder_model, n_epochs=100, n_batch=32):\n",
    "    batches_per_epoch = int(60000 / n_batch)\n",
    "\n",
    "    for epoch in range(0, n_epochs):\n",
    "        for batch in range(0, batches_per_epoch):\n",
    "\n",
    "            # Get MNIST images\n",
    "            train_images, y_train_images = get_MNIST_samples(n_batch)\n",
    "\n",
    "            # Run a batch through the autoencoder_model\n",
    "            autoencoder_loss = autoencoder_model.train_on_batch(train_images, y_train_images)\n",
    "\n",
    "            print('Epoch: ', epoch, '   Batch Number: ', batch, ' / ', batches_per_epoch, '   Autoencoder loss: ',  autoencoder_loss)\n",
    "\n",
    "            # Save values and models\n",
    "            if batch % 1800 == 0:\n",
    "                # Save the generator and discriminator\n",
    "                encoder.save('encoder.h5', )\n",
    "                decoder.save('decoder.h5')\n",
    "                autoencoder_model.save('autoencoder_model.h5')\n",
    "\n",
    "                # Predict some images to read out the progress of the autoencoder\n",
    "                autoencoder_model_output = autoencoder_model.predict(train_images)\n",
    "\n",
    "                # Print generated_images with matplotlib\n",
    "                # First the images from train_images, then autoencoder_moder_output\n",
    "                for i in range(1, 5):\n",
    "                    # Define subplot of size 5 x 5\n",
    "                    plt.subplot(4, 4, i + 3 * (i - 1))\n",
    "                    if i == 1:\n",
    "                        plt.title(\"MNIST\")\n",
    "                    # Plot raw pixel data\n",
    "                    plt.imshow(train_images[i, :, :, 0], cmap='gray_r')\n",
    "\n",
    "                    # Define subplot of size 5 x 5\n",
    "                    plt.subplot(4, 4, i + 1 + 3 * (i - 1))\n",
    "                    if i == 1:\n",
    "                        plt.title(\"Generated\")\n",
    "                    # Plot raw pixel data\n",
    "                    plt.imshow(autoencoder_model_output[i, :, :, 0], cmap='gray_r')\n",
    "\n",
    "                    # Define subplot of size 5 x 5\n",
    "                    plt.subplot(4, 4, i + 2 + 3 * (i - 1))\n",
    "                    if i == 1:\n",
    "                        plt.title(\"MNIST\")\n",
    "                    # Plot raw pixel data\n",
    "                    plt.imshow(train_images[i + 4, :, :, 0], cmap='gray_r')\n",
    "\n",
    "                    # Define subplot of size 5 x 5\n",
    "                    plt.subplot(4, 4, i + 3 + 3 * (i - 1))\n",
    "                    if i == 1:\n",
    "                        plt.title(\"Generated\")\n",
    "                    # Plot raw pixel data\n",
    "                    plt.imshow(autoencoder_model_output[i + 4, :, :, 0], cmap='gray_r')\n",
    "\n",
    "                plt.savefig('plot  epoch ' + '%02d' % epoch + '  batch ' + '%05d' % batch + '.png', dpi=600)\n",
    "                plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0    Batch Number:  0  /  1875    Autoencoder loss:  0.693195641040802\n",
      "Epoch:  0    Batch Number:  1  /  1875    Autoencoder loss:  0.6925737857818604\n",
      "Epoch:  0    Batch Number:  2  /  1875    Autoencoder loss:  0.6917673349380493\n",
      "Epoch:  0    Batch Number:  3  /  1875    Autoencoder loss:  0.689876139163971\n",
      "Epoch:  0    Batch Number:  4  /  1875    Autoencoder loss:  0.6848605275154114\n",
      "Epoch:  0    Batch Number:  5  /  1875    Autoencoder loss:  0.6749283671379089\n",
      "Epoch:  0    Batch Number:  6  /  1875    Autoencoder loss:  0.6561025381088257\n",
      "Epoch:  0    Batch Number:  7  /  1875    Autoencoder loss:  0.6273826360702515\n",
      "Epoch:  0    Batch Number:  8  /  1875    Autoencoder loss:  0.5830464959144592\n",
      "Epoch:  0    Batch Number:  9  /  1875    Autoencoder loss:  0.5309769511222839\n",
      "Epoch:  0    Batch Number:  10  /  1875    Autoencoder loss:  0.46627214550971985\n",
      "Epoch:  0    Batch Number:  11  /  1875    Autoencoder loss:  0.3979540765285492\n",
      "Epoch:  0    Batch Number:  12  /  1875    Autoencoder loss:  0.38373085856437683\n",
      "Epoch:  0    Batch Number:  13  /  1875    Autoencoder loss:  0.3870547413825989\n",
      "Epoch:  0    Batch Number:  14  /  1875    Autoencoder loss:  0.34015122056007385\n",
      "Epoch:  0    Batch Number:  15  /  1875    Autoencoder loss:  0.3597356975078583\n",
      "Epoch:  0    Batch Number:  16  /  1875    Autoencoder loss:  0.34963828325271606\n",
      "Epoch:  0    Batch Number:  17  /  1875    Autoencoder loss:  0.31174346804618835\n",
      "Epoch:  0    Batch Number:  18  /  1875    Autoencoder loss:  0.2973000407218933\n",
      "Epoch:  0    Batch Number:  19  /  1875    Autoencoder loss:  0.2960561513900757\n",
      "Epoch:  0    Batch Number:  20  /  1875    Autoencoder loss:  0.2952052652835846\n",
      "Epoch:  0    Batch Number:  21  /  1875    Autoencoder loss:  0.284094899892807\n",
      "Epoch:  0    Batch Number:  22  /  1875    Autoencoder loss:  0.2850697636604309\n",
      "Epoch:  0    Batch Number:  23  /  1875    Autoencoder loss:  0.2995612621307373\n",
      "Epoch:  0    Batch Number:  24  /  1875    Autoencoder loss:  0.28769364953041077\n",
      "Epoch:  0    Batch Number:  25  /  1875    Autoencoder loss:  0.30298128724098206\n",
      "Epoch:  0    Batch Number:  26  /  1875    Autoencoder loss:  0.2996114492416382\n",
      "Epoch:  0    Batch Number:  27  /  1875    Autoencoder loss:  0.29618000984191895\n",
      "Epoch:  0    Batch Number:  28  /  1875    Autoencoder loss:  0.2994334101676941\n",
      "Epoch:  0    Batch Number:  29  /  1875    Autoencoder loss:  0.3126313090324402\n",
      "Epoch:  0    Batch Number:  30  /  1875    Autoencoder loss:  0.2941157817840576\n",
      "Epoch:  0    Batch Number:  31  /  1875    Autoencoder loss:  0.2805751860141754\n",
      "Epoch:  0    Batch Number:  32  /  1875    Autoencoder loss:  0.2660464346408844\n",
      "Epoch:  0    Batch Number:  33  /  1875    Autoencoder loss:  0.2738989591598511\n",
      "Epoch:  0    Batch Number:  34  /  1875    Autoencoder loss:  0.2757475674152374\n",
      "Epoch:  0    Batch Number:  35  /  1875    Autoencoder loss:  0.2608049809932709\n",
      "Epoch:  0    Batch Number:  36  /  1875    Autoencoder loss:  0.265277624130249\n",
      "Epoch:  0    Batch Number:  37  /  1875    Autoencoder loss:  0.2842862606048584\n",
      "Epoch:  0    Batch Number:  38  /  1875    Autoencoder loss:  0.2678597867488861\n",
      "Epoch:  0    Batch Number:  39  /  1875    Autoencoder loss:  0.2846378684043884\n",
      "Epoch:  0    Batch Number:  40  /  1875    Autoencoder loss:  0.2577762305736542\n",
      "Epoch:  0    Batch Number:  41  /  1875    Autoencoder loss:  0.26592710614204407\n",
      "Epoch:  0    Batch Number:  42  /  1875    Autoencoder loss:  0.2695545554161072\n",
      "Epoch:  0    Batch Number:  43  /  1875    Autoencoder loss:  0.2795315682888031\n",
      "Epoch:  0    Batch Number:  44  /  1875    Autoencoder loss:  0.2708252966403961\n",
      "Epoch:  0    Batch Number:  45  /  1875    Autoencoder loss:  0.2761225998401642\n",
      "Epoch:  0    Batch Number:  46  /  1875    Autoencoder loss:  0.2694287896156311\n",
      "Epoch:  0    Batch Number:  47  /  1875    Autoencoder loss:  0.27238523960113525\n",
      "Epoch:  0    Batch Number:  48  /  1875    Autoencoder loss:  0.2636086940765381\n",
      "Epoch:  0    Batch Number:  49  /  1875    Autoencoder loss:  0.286409467458725\n",
      "Epoch:  0    Batch Number:  50  /  1875    Autoencoder loss:  0.2771761119365692\n",
      "Epoch:  0    Batch Number:  51  /  1875    Autoencoder loss:  0.28468212485313416\n",
      "Epoch:  0    Batch Number:  52  /  1875    Autoencoder loss:  0.23260746896266937\n",
      "Epoch:  0    Batch Number:  53  /  1875    Autoencoder loss:  0.26571136713027954\n",
      "Epoch:  0    Batch Number:  54  /  1875    Autoencoder loss:  0.2766457498073578\n",
      "Epoch:  0    Batch Number:  55  /  1875    Autoencoder loss:  0.25487688183784485\n",
      "Epoch:  0    Batch Number:  56  /  1875    Autoencoder loss:  0.2555401921272278\n",
      "Epoch:  0    Batch Number:  57  /  1875    Autoencoder loss:  0.2491917461156845\n",
      "Epoch:  0    Batch Number:  58  /  1875    Autoencoder loss:  0.2564755082130432\n",
      "Epoch:  0    Batch Number:  59  /  1875    Autoencoder loss:  0.27434730529785156\n",
      "Epoch:  0    Batch Number:  60  /  1875    Autoencoder loss:  0.2689454257488251\n",
      "Epoch:  0    Batch Number:  61  /  1875    Autoencoder loss:  0.24620044231414795\n",
      "Epoch:  0    Batch Number:  62  /  1875    Autoencoder loss:  0.25546765327453613\n",
      "Epoch:  0    Batch Number:  63  /  1875    Autoencoder loss:  0.26899799704551697\n",
      "Epoch:  0    Batch Number:  64  /  1875    Autoencoder loss:  0.2563292384147644\n",
      "Epoch:  0    Batch Number:  65  /  1875    Autoencoder loss:  0.2633533179759979\n",
      "Epoch:  0    Batch Number:  66  /  1875    Autoencoder loss:  0.2524716556072235\n",
      "Epoch:  0    Batch Number:  67  /  1875    Autoencoder loss:  0.2743760049343109\n",
      "Epoch:  0    Batch Number:  68  /  1875    Autoencoder loss:  0.2554958164691925\n",
      "Epoch:  0    Batch Number:  69  /  1875    Autoencoder loss:  0.2635377049446106\n",
      "Epoch:  0    Batch Number:  70  /  1875    Autoencoder loss:  0.24091467261314392\n",
      "Epoch:  0    Batch Number:  71  /  1875    Autoencoder loss:  0.28469860553741455\n",
      "Epoch:  0    Batch Number:  72  /  1875    Autoencoder loss:  0.26075711846351624\n",
      "Epoch:  0    Batch Number:  73  /  1875    Autoencoder loss:  0.27813127636909485\n",
      "Epoch:  0    Batch Number:  74  /  1875    Autoencoder loss:  0.2460414320230484\n",
      "Epoch:  0    Batch Number:  75  /  1875    Autoencoder loss:  0.27831679582595825\n",
      "Epoch:  0    Batch Number:  76  /  1875    Autoencoder loss:  0.25042200088500977\n",
      "Epoch:  0    Batch Number:  77  /  1875    Autoencoder loss:  0.26907846331596375\n",
      "Epoch:  0    Batch Number:  78  /  1875    Autoencoder loss:  0.2580954432487488\n",
      "Epoch:  0    Batch Number:  79  /  1875    Autoencoder loss:  0.2553600072860718\n",
      "Epoch:  0    Batch Number:  80  /  1875    Autoencoder loss:  0.2531593441963196\n",
      "Epoch:  0    Batch Number:  81  /  1875    Autoencoder loss:  0.2683180272579193\n",
      "Epoch:  0    Batch Number:  82  /  1875    Autoencoder loss:  0.26484808325767517\n",
      "Epoch:  0    Batch Number:  83  /  1875    Autoencoder loss:  0.24911187589168549\n",
      "Epoch:  0    Batch Number:  84  /  1875    Autoencoder loss:  0.24652792513370514\n",
      "Epoch:  0    Batch Number:  85  /  1875    Autoencoder loss:  0.27617886662483215\n",
      "Epoch:  0    Batch Number:  86  /  1875    Autoencoder loss:  0.25131553411483765\n",
      "Epoch:  0    Batch Number:  87  /  1875    Autoencoder loss:  0.2514656186103821\n",
      "Epoch:  0    Batch Number:  88  /  1875    Autoencoder loss:  0.2600387930870056\n",
      "Epoch:  0    Batch Number:  89  /  1875    Autoencoder loss:  0.2598983347415924\n",
      "Epoch:  0    Batch Number:  90  /  1875    Autoencoder loss:  0.24223721027374268\n",
      "Epoch:  0    Batch Number:  91  /  1875    Autoencoder loss:  0.2402268946170807\n",
      "Epoch:  0    Batch Number:  92  /  1875    Autoencoder loss:  0.27582642436027527\n",
      "Epoch:  0    Batch Number:  93  /  1875    Autoencoder loss:  0.24922841787338257\n",
      "Epoch:  0    Batch Number:  94  /  1875    Autoencoder loss:  0.24102522432804108\n",
      "Epoch:  0    Batch Number:  95  /  1875    Autoencoder loss:  0.24807630479335785\n",
      "Epoch:  0    Batch Number:  96  /  1875    Autoencoder loss:  0.25257426500320435\n",
      "Epoch:  0    Batch Number:  97  /  1875    Autoencoder loss:  0.24972930550575256\n",
      "Epoch:  0    Batch Number:  98  /  1875    Autoencoder loss:  0.2585783898830414\n",
      "Epoch:  0    Batch Number:  99  /  1875    Autoencoder loss:  0.2643619179725647\n",
      "Epoch:  0    Batch Number:  100  /  1875    Autoencoder loss:  0.2460724115371704\n",
      "Epoch:  0    Batch Number:  101  /  1875    Autoencoder loss:  0.26788097620010376\n",
      "Epoch:  0    Batch Number:  102  /  1875    Autoencoder loss:  0.26822707056999207\n",
      "Epoch:  0    Batch Number:  103  /  1875    Autoencoder loss:  0.25562137365341187\n",
      "Epoch:  0    Batch Number:  104  /  1875    Autoencoder loss:  0.26657435297966003\n",
      "Epoch:  0    Batch Number:  105  /  1875    Autoencoder loss:  0.2292277216911316\n",
      "Epoch:  0    Batch Number:  106  /  1875    Autoencoder loss:  0.25008371472358704\n",
      "Epoch:  0    Batch Number:  107  /  1875    Autoencoder loss:  0.22860702872276306\n",
      "Epoch:  0    Batch Number:  108  /  1875    Autoencoder loss:  0.2567537724971771\n",
      "Epoch:  0    Batch Number:  109  /  1875    Autoencoder loss:  0.23614661395549774\n",
      "Epoch:  0    Batch Number:  110  /  1875    Autoencoder loss:  0.23412016034126282\n",
      "Epoch:  0    Batch Number:  111  /  1875    Autoencoder loss:  0.24569125473499298\n",
      "Epoch:  0    Batch Number:  112  /  1875    Autoencoder loss:  0.23958301544189453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0    Batch Number:  113  /  1875    Autoencoder loss:  0.2625458538532257\n",
      "Epoch:  0    Batch Number:  114  /  1875    Autoencoder loss:  0.245770663022995\n",
      "Epoch:  0    Batch Number:  115  /  1875    Autoencoder loss:  0.23679618537425995\n",
      "Epoch:  0    Batch Number:  116  /  1875    Autoencoder loss:  0.2610098719596863\n",
      "Epoch:  0    Batch Number:  117  /  1875    Autoencoder loss:  0.24401800334453583\n",
      "Epoch:  0    Batch Number:  118  /  1875    Autoencoder loss:  0.26199597120285034\n",
      "Epoch:  0    Batch Number:  119  /  1875    Autoencoder loss:  0.2458365261554718\n",
      "Epoch:  0    Batch Number:  120  /  1875    Autoencoder loss:  0.26699668169021606\n",
      "Epoch:  0    Batch Number:  121  /  1875    Autoencoder loss:  0.23909778892993927\n",
      "Epoch:  0    Batch Number:  122  /  1875    Autoencoder loss:  0.2561016380786896\n",
      "Epoch:  0    Batch Number:  123  /  1875    Autoencoder loss:  0.2382844090461731\n",
      "Epoch:  0    Batch Number:  124  /  1875    Autoencoder loss:  0.2716597020626068\n",
      "Epoch:  0    Batch Number:  125  /  1875    Autoencoder loss:  0.2502177953720093\n",
      "Epoch:  0    Batch Number:  126  /  1875    Autoencoder loss:  0.23262786865234375\n",
      "Epoch:  0    Batch Number:  127  /  1875    Autoencoder loss:  0.25111398100852966\n",
      "Epoch:  0    Batch Number:  128  /  1875    Autoencoder loss:  0.2561914026737213\n",
      "Epoch:  0    Batch Number:  129  /  1875    Autoencoder loss:  0.25134244561195374\n",
      "Epoch:  0    Batch Number:  130  /  1875    Autoencoder loss:  0.26121410727500916\n",
      "Epoch:  0    Batch Number:  131  /  1875    Autoencoder loss:  0.24622909724712372\n",
      "Epoch:  0    Batch Number:  132  /  1875    Autoencoder loss:  0.2592357397079468\n",
      "Epoch:  0    Batch Number:  133  /  1875    Autoencoder loss:  0.23961542546749115\n",
      "Epoch:  0    Batch Number:  134  /  1875    Autoencoder loss:  0.2556416392326355\n",
      "Epoch:  0    Batch Number:  135  /  1875    Autoencoder loss:  0.2354646474123001\n",
      "Epoch:  0    Batch Number:  136  /  1875    Autoencoder loss:  0.25318193435668945\n",
      "Epoch:  0    Batch Number:  137  /  1875    Autoencoder loss:  0.26096639037132263\n",
      "Epoch:  0    Batch Number:  138  /  1875    Autoencoder loss:  0.2369062900543213\n",
      "Epoch:  0    Batch Number:  139  /  1875    Autoencoder loss:  0.24797064065933228\n",
      "Epoch:  0    Batch Number:  140  /  1875    Autoencoder loss:  0.22852225601673126\n",
      "Epoch:  0    Batch Number:  141  /  1875    Autoencoder loss:  0.2559800148010254\n",
      "Epoch:  0    Batch Number:  142  /  1875    Autoencoder loss:  0.24064432084560394\n",
      "Epoch:  0    Batch Number:  143  /  1875    Autoencoder loss:  0.2748600244522095\n",
      "Epoch:  0    Batch Number:  144  /  1875    Autoencoder loss:  0.23384808003902435\n",
      "Epoch:  0    Batch Number:  145  /  1875    Autoencoder loss:  0.22821305692195892\n",
      "Epoch:  0    Batch Number:  146  /  1875    Autoencoder loss:  0.25775548815727234\n",
      "Epoch:  0    Batch Number:  147  /  1875    Autoencoder loss:  0.25735700130462646\n",
      "Epoch:  0    Batch Number:  148  /  1875    Autoencoder loss:  0.22914408147335052\n",
      "Epoch:  0    Batch Number:  149  /  1875    Autoencoder loss:  0.2515348792076111\n",
      "Epoch:  0    Batch Number:  150  /  1875    Autoencoder loss:  0.2531355917453766\n",
      "Epoch:  0    Batch Number:  151  /  1875    Autoencoder loss:  0.26875391602516174\n",
      "Epoch:  0    Batch Number:  152  /  1875    Autoencoder loss:  0.23225508630275726\n",
      "Epoch:  0    Batch Number:  153  /  1875    Autoencoder loss:  0.24607065320014954\n",
      "Epoch:  0    Batch Number:  154  /  1875    Autoencoder loss:  0.23896777629852295\n",
      "Epoch:  0    Batch Number:  155  /  1875    Autoencoder loss:  0.25259631872177124\n",
      "Epoch:  0    Batch Number:  156  /  1875    Autoencoder loss:  0.22094911336898804\n",
      "Epoch:  0    Batch Number:  157  /  1875    Autoencoder loss:  0.2590513825416565\n",
      "Epoch:  0    Batch Number:  158  /  1875    Autoencoder loss:  0.2362581342458725\n",
      "Epoch:  0    Batch Number:  159  /  1875    Autoencoder loss:  0.25639766454696655\n",
      "Epoch:  0    Batch Number:  160  /  1875    Autoencoder loss:  0.2379361391067505\n",
      "Epoch:  0    Batch Number:  161  /  1875    Autoencoder loss:  0.24374711513519287\n",
      "Epoch:  0    Batch Number:  162  /  1875    Autoencoder loss:  0.21897561848163605\n",
      "Epoch:  0    Batch Number:  163  /  1875    Autoencoder loss:  0.22697192430496216\n",
      "Epoch:  0    Batch Number:  164  /  1875    Autoencoder loss:  0.23320865631103516\n",
      "Epoch:  0    Batch Number:  165  /  1875    Autoencoder loss:  0.2152288854122162\n",
      "Epoch:  0    Batch Number:  166  /  1875    Autoencoder loss:  0.24423880875110626\n",
      "Epoch:  0    Batch Number:  167  /  1875    Autoencoder loss:  0.2455972135066986\n",
      "Epoch:  0    Batch Number:  168  /  1875    Autoencoder loss:  0.26240274310112\n",
      "Epoch:  0    Batch Number:  169  /  1875    Autoencoder loss:  0.24869723618030548\n",
      "Epoch:  0    Batch Number:  170  /  1875    Autoencoder loss:  0.220870703458786\n",
      "Epoch:  0    Batch Number:  171  /  1875    Autoencoder loss:  0.22271767258644104\n",
      "Epoch:  0    Batch Number:  172  /  1875    Autoencoder loss:  0.25386685132980347\n",
      "Epoch:  0    Batch Number:  173  /  1875    Autoencoder loss:  0.23832827806472778\n",
      "Epoch:  0    Batch Number:  174  /  1875    Autoencoder loss:  0.2422788143157959\n",
      "Epoch:  0    Batch Number:  175  /  1875    Autoencoder loss:  0.2402125746011734\n",
      "Epoch:  0    Batch Number:  176  /  1875    Autoencoder loss:  0.2408130019903183\n",
      "Epoch:  0    Batch Number:  177  /  1875    Autoencoder loss:  0.22146236896514893\n",
      "Epoch:  0    Batch Number:  178  /  1875    Autoencoder loss:  0.2706618010997772\n",
      "Epoch:  0    Batch Number:  179  /  1875    Autoencoder loss:  0.22418949007987976\n",
      "Epoch:  0    Batch Number:  180  /  1875    Autoencoder loss:  0.24032248556613922\n",
      "Epoch:  0    Batch Number:  181  /  1875    Autoencoder loss:  0.23811422288417816\n",
      "Epoch:  0    Batch Number:  182  /  1875    Autoencoder loss:  0.2325681895017624\n",
      "Epoch:  0    Batch Number:  183  /  1875    Autoencoder loss:  0.24047096073627472\n",
      "Epoch:  0    Batch Number:  184  /  1875    Autoencoder loss:  0.26036664843559265\n",
      "Epoch:  0    Batch Number:  185  /  1875    Autoencoder loss:  0.23300516605377197\n",
      "Epoch:  0    Batch Number:  186  /  1875    Autoencoder loss:  0.2464018613100052\n",
      "Epoch:  0    Batch Number:  187  /  1875    Autoencoder loss:  0.24270036816596985\n",
      "Epoch:  0    Batch Number:  188  /  1875    Autoencoder loss:  0.22523899376392365\n",
      "Epoch:  0    Batch Number:  189  /  1875    Autoencoder loss:  0.24296212196350098\n",
      "Epoch:  0    Batch Number:  190  /  1875    Autoencoder loss:  0.22707301378250122\n",
      "Epoch:  0    Batch Number:  191  /  1875    Autoencoder loss:  0.22975832223892212\n",
      "Epoch:  0    Batch Number:  192  /  1875    Autoencoder loss:  0.22294287383556366\n",
      "Epoch:  0    Batch Number:  193  /  1875    Autoencoder loss:  0.24639777839183807\n",
      "Epoch:  0    Batch Number:  194  /  1875    Autoencoder loss:  0.23512524366378784\n",
      "Epoch:  0    Batch Number:  195  /  1875    Autoencoder loss:  0.2274966537952423\n",
      "Epoch:  0    Batch Number:  196  /  1875    Autoencoder loss:  0.2572978138923645\n",
      "Epoch:  0    Batch Number:  197  /  1875    Autoencoder loss:  0.22864878177642822\n",
      "Epoch:  0    Batch Number:  198  /  1875    Autoencoder loss:  0.24509862065315247\n",
      "Epoch:  0    Batch Number:  199  /  1875    Autoencoder loss:  0.23463287949562073\n",
      "Epoch:  0    Batch Number:  200  /  1875    Autoencoder loss:  0.23517495393753052\n",
      "Epoch:  0    Batch Number:  201  /  1875    Autoencoder loss:  0.21371375024318695\n",
      "Epoch:  0    Batch Number:  202  /  1875    Autoencoder loss:  0.23990245163440704\n",
      "Epoch:  0    Batch Number:  203  /  1875    Autoencoder loss:  0.25075268745422363\n",
      "Epoch:  0    Batch Number:  204  /  1875    Autoencoder loss:  0.2423614263534546\n",
      "Epoch:  0    Batch Number:  205  /  1875    Autoencoder loss:  0.20735007524490356\n",
      "Epoch:  0    Batch Number:  206  /  1875    Autoencoder loss:  0.2335716187953949\n",
      "Epoch:  0    Batch Number:  207  /  1875    Autoencoder loss:  0.23533645272254944\n",
      "Epoch:  0    Batch Number:  208  /  1875    Autoencoder loss:  0.23558107018470764\n",
      "Epoch:  0    Batch Number:  209  /  1875    Autoencoder loss:  0.22031395137310028\n",
      "Epoch:  0    Batch Number:  210  /  1875    Autoencoder loss:  0.24417611956596375\n",
      "Epoch:  0    Batch Number:  211  /  1875    Autoencoder loss:  0.22295524179935455\n",
      "Epoch:  0    Batch Number:  212  /  1875    Autoencoder loss:  0.24109242856502533\n",
      "Epoch:  0    Batch Number:  213  /  1875    Autoencoder loss:  0.23246757686138153\n",
      "Epoch:  0    Batch Number:  214  /  1875    Autoencoder loss:  0.21314041316509247\n",
      "Epoch:  0    Batch Number:  215  /  1875    Autoencoder loss:  0.24349452555179596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0    Batch Number:  216  /  1875    Autoencoder loss:  0.23982098698616028\n",
      "Epoch:  0    Batch Number:  217  /  1875    Autoencoder loss:  0.23133142292499542\n",
      "Epoch:  0    Batch Number:  218  /  1875    Autoencoder loss:  0.24833150207996368\n",
      "Epoch:  0    Batch Number:  219  /  1875    Autoencoder loss:  0.24040257930755615\n",
      "Epoch:  0    Batch Number:  220  /  1875    Autoencoder loss:  0.23645029962062836\n",
      "Epoch:  0    Batch Number:  221  /  1875    Autoencoder loss:  0.2422371357679367\n",
      "Epoch:  0    Batch Number:  222  /  1875    Autoencoder loss:  0.2108052521944046\n",
      "Epoch:  0    Batch Number:  223  /  1875    Autoencoder loss:  0.226809561252594\n",
      "Epoch:  0    Batch Number:  224  /  1875    Autoencoder loss:  0.23237434029579163\n",
      "Epoch:  0    Batch Number:  225  /  1875    Autoencoder loss:  0.24104531109333038\n",
      "Epoch:  0    Batch Number:  226  /  1875    Autoencoder loss:  0.23854924738407135\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-7c9fc3a6b360>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# Train Autoencoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mautoencoder_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-24-1fa763d92b42>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(encoder, decoder, autoencoder_model, n_epochs, n_batch)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m             \u001b[1;31m# Run a batch through the autoencoder_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m             \u001b[0mautoencoder_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mautoencoder_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_images\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Epoch: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'   Batch Number: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m' / '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatches_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'   Autoencoder loss: '\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mautoencoder_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\Notebook\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[0;32m   1349\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1350\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1351\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1352\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_numpy_or_python_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1353\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\Notebook\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mreset_metrics\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1289\u001b[0m     \u001b[1;34m\"\"\"Resets the state of metrics.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1290\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1291\u001b[1;33m       \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_states\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1292\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1293\u001b[0m   def train_on_batch(self,\n",
      "\u001b[1;32m~\\.conda\\envs\\Notebook\\lib\\site-packages\\tensorflow\\python\\keras\\metrics.py\u001b[0m in \u001b[0;36mreset_states\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    222\u001b[0m     \u001b[0mwhen\u001b[0m \u001b[0ma\u001b[0m \u001b[0mmetric\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mevaluated\u001b[0m \u001b[0mduring\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m     \"\"\"\n\u001b[1;32m--> 224\u001b[1;33m     \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_set_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mabc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\Notebook\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36mbatch_set_value\u001b[1;34m(tuples)\u001b[0m\n\u001b[0;32m   3382\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly_outside_functions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3383\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtuples\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3384\u001b[1;33m       \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massign\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3385\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3386\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mget_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\Notebook\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\u001b[0m in \u001b[0;36massign\u001b[1;34m(self, value, use_locking, name, read_value)\u001b[0m\n\u001b[0;32m    846\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_shape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massert_is_compatible_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue_tensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    847\u001b[0m       assign_op = gen_resource_variable_ops.assign_variable_op(\n\u001b[1;32m--> 848\u001b[1;33m           self.handle, value_tensor, name=name)\n\u001b[0m\u001b[0;32m    849\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mread_value\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    850\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lazy_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0massign_op\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\Notebook\\lib\\site-packages\\tensorflow\\python\\ops\\gen_resource_variable_ops.py\u001b[0m in \u001b[0;36massign_variable_op\u001b[1;34m(resource, value, name)\u001b[0m\n\u001b[0;32m    140\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0;32m    141\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"AssignVariableOp\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m         tld.op_callbacks, resource, value)\n\u001b[0m\u001b[0;32m    143\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Start training cycle\n",
    "\n",
    "# Build the autoencoder\n",
    "encoder = build_encoder()\n",
    "decoder = build_decoder()\n",
    "autoencoder_model = build_autoencoder(encoder, decoder)\n",
    "\n",
    "# Train Autoencoder\n",
    "train(encoder, decoder, autoencoder_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Notebook",
   "language": "python",
   "name": "notebook"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
